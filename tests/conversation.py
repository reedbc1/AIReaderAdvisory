import json
import numpy as np
import faiss
from openai import OpenAI
from dotenv import load_dotenv
import os
import logging

# Basic configuration (sets up a StreamHandler and Formatter for the root logger)
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# Get a named logger
logger = logging.getLogger(__name__)

# Load API key
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Load index and data
index = faiss.read_index("library.index")

with open("json_files/wr_enhanced.json", encoding="utf-8") as f:
    data = json.load(f)

embeddings = np.load("library_embeddings.npy")
assert len(data) == embeddings.shape[
    0], "‚ùå Mismatch between JSON records and embeddings!"

# 1. Define a list of callable tools for the model
tools = [{
    "type": "web_search"
}, {
    "type": "function",
    "name": "search_library",
    "description": "Find similar movies to user request.",
    "strict": True,
    "parameters": {
        "type": "object",
        "properties": {
            "query": {
                "type":
                "string",
                "description":
                ("A description of what kinds of movies the customer is looking for,"
                 "including the names of movies and actors.")
            },
            "k": {
                "type": "integer",
                "description": "The number of top similar results to return.",
                "default": 5
            }
        },
        "required": ["query", "k"],
        "additionalProperties": False
    }
}]


def search_library(query, k):
    """Return top-k most similar library items to a text query."""

    # Get embedding for the query
    response = client.embeddings.create(model="text-embedding-3-small",
                                        input=query)

    query_vec = np.array(response.data[0].embedding,
                         dtype="float32").reshape(1, -1)

    # Search FAISS
    distances, indices = index.search(query_vec, k)

    # Return top results with metadata
    results = []
    for dist, idx in zip(distances[0], indices[0]):
        record = data[idx]
        # add link so that the tool can link to vega for items.
        results.append({
            "title": record.get("title"),
            "author": record.get("author"),
            "material": record.get("materials", [])[0].get("name"),
            "year": record.get("publicationDate"),
            "summary": record.get("summary"),
            "subjects": record.get("subjects"),
            "contributors": record.get("contributors"),
            "distance": float(dist)
        })
    return results


def call_function(name, args):
    if name == "search_library":
        return search_library(**args)


def create_conversation():
    conversation = client.conversations.create()
    return conversation.id


logger.info("creating conversation...")
conv_id = create_conversation()
# logger.info(f"conv_id: {conv_id}")

logger.info("starting loop...")
while True:
    query = str(input("Enter query: "))
    if query == "exit":
        break

    input_messages = [{"role": "user", "content": f"{query}"}]

    # 2. Prompt the model with tools defined
    logger.info("getting parameters...")
    response = client.responses.create(model="gpt-5",
                                       tools=tools,
                                       input=input_messages,
                                       conversation=conv_id)

    for tool_call in response.output:
        if tool_call.type == "function_call":

            name = tool_call.name
            args = json.loads(tool_call.arguments)
            logger.info("function call found!")
            logger.info(f"name: {name}")
            logger.info(f"args: {args}")

            logger.info("calling function with name, args...")
            result = call_function(name, args)
            # append to conversation object
            logger.info("creating output item in conversation...")
            client.conversations.items.create(
                conv_id,
                items=[{
                    "type": "function_call_output",
                    "call_id": tool_call.call_id,
                    # json.dumps(result) ?
                    "output": str(result)
                }])
            # logger.info("showing conversation items:")
            # items = client.conversations.items.list(conv_id, limit=10)
            # logger.info(items.data)

            logger.info("picking 3 movies...")
            response = client.responses.create(
                model="gpt-5",
                input=
                "pick 3 of the movies generated by a tool and explain how they match the query.",
                conversation=conv_id)

    print(response.output_text)
